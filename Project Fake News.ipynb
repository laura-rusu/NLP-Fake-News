{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <b> </b> | <b></b> |\n",
    "|--|--|\n",
    "| **Group** | *22* |\n",
    "| **Group member 1** | *Alfred Schell* |\n",
    "| **Group member 2** | *Laura Rusu* |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main idea:\n",
    "We will follow 5 main steps:\n",
    "1. Loading the data\n",
    "2. Preprocessing                                                                               \n",
    "2.1 Cleaning the datasets (concatenating the title and the text, creating a single dataset from the two)                  \n",
    "2.2 Tokenization (we will use the NLTK tokenizer)                                                                                \n",
    "2.3 Lemmatization                                                                                \n",
    "2.4 Splitting into unigrams and bigrams (depending on the chosen model)                                                                               \n",
    "2.5 Removing stopwords (this will be employed in case our accuracy proves to be insufficient)\n",
    "3. Vectorizing the data\n",
    "4. Employing a model to train the data (multiple models will be used to test the various accuracies)                                                                               \n",
    "4.1 Logistic regression model                                                                               \n",
    "4.2 Naive Bayes                                                                               \n",
    "4.3 Support Vector Machines                                                                               \n",
    "4.4 Unigram language model                                                                               \n",
    "4.5 Bigram language model\n",
    "5. Checking the accuracy for each variation, to decide on the best option for the given problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "from collections import Counter\n",
    "#from pandas import series\n",
    "\n",
    "#NLTK libraries\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "from itertools import chain\n",
    "\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Machine Learning libraries\n",
    "import sklearn \n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "#Metrics libraries\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.metrics import roc_auc_score\n",
    "#from sklearn.metrics import roc_curve\n",
    "\n",
    "#Miscellanous libraries\n",
    "\n",
    "\n",
    "#Ignore warnings\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the  data is (row, column):(23481, 4)\n",
      "The shape of the  data is (row, column):(21417, 4)\n"
     ]
    }
   ],
   "source": [
    "#reading the fake and true datasets\n",
    "fake_news = pd.read_csv('Fake.csv')\n",
    "true_news = pd.read_csv('True.csv')\n",
    "\n",
    "# print shape of the datasets\n",
    "print (\"The shape of the  data is (row, column):\"+ str(fake_news.shape)) #(23481, 4) - contains 23481 fake news articles\n",
    "print (\"The shape of the  data is (row, column):\"+ str(true_news.shape)) #(21417, 4) - contains 21417 true news articles\n",
    "\n",
    "#Either category contains a title (the news headline), text (the article), subject (type of news) \n",
    "#and date (when the news was published)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cleaning the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>News</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject               date  \\\n",
       "0    News  December 31, 2017   \n",
       "1    News  December 31, 2017   \n",
       "2    News  December 30, 2017   \n",
       "3    News  December 29, 2017   \n",
       "4    News  December 25, 2017   \n",
       "\n",
       "                                                News  Output  \n",
       "0   Donald Trump Sends Out Embarrassing New Year’...       0  \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...       0  \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...       0  \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...       0  \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...       0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating target variables\n",
    "fake_news['Output'] = 0\n",
    "true_news['Output'] = 1\n",
    "\n",
    "#Concatenating title and article text\n",
    "fake_news['News'] = fake_news['title'] + fake_news['text']\n",
    "true_news['News'] = true_news['title'] + true_news['text']\n",
    "\n",
    "#Removing the 'title' and 'text' columns\n",
    "fake_news = fake_news.drop(['title', 'text'], axis=1)\n",
    "true_news = true_news.drop(['title', 'text'], axis=1)\n",
    "\n",
    "#Rearranging the columns\n",
    "fake_news = fake_news[['subject', 'date', 'News','Output']]\n",
    "true_news = true_news[['subject', 'date', 'News','Output']]\n",
    "\n",
    "both_news = [fake_news, true_news]\n",
    "news_dataset = pd.concat(both_news)\n",
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will be using an off-the-shelf tokenizer\n",
    "\n",
    "sp()\n",
    "def tokenize_news(news):  \n",
    "    tokenized_news = [] \n",
    "    for i in news:\n",
    "        tokenized_str=(word_tokenize(i))\n",
    "        \n",
    "        #Remove punctuation marks\n",
    "        for j in range(len(tokenized_str)):\n",
    "            tokenized_str[j]=tokenized_str[j].translate(str.maketrans('','',string.punctuation))\n",
    "            \n",
    "        #Remove empty spaces\n",
    "        tokenized_str[:] = [x for x in tokenized_str if x]\n",
    "        \n",
    "        #Transform everything to lowercase \n",
    "        tokenized_str[:] =[x.lower() for x in tokenized_str]\n",
    "        \n",
    "        #Add start and end token \n",
    "        tokenized_str[:] =['<S>'] + tokenized_str[:] +['<E>']\n",
    "        tokenized_news.append(tokenized_str)\n",
    "        \n",
    "    return tokenized_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>News</th>\n",
       "      <th>Output</th>\n",
       "      <th>Tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>0</td>\n",
       "      <td>[&lt;S&gt;, donald, trump, sends, out, embarrassing,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[&lt;S&gt;, drunk, bragging, trump, staffer, started...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>0</td>\n",
       "      <td>[&lt;S&gt;, sheriff, david, clarke, becomes, an, int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>0</td>\n",
       "      <td>[&lt;S&gt;, trump, is, so, obsessed, he, even, has, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>0</td>\n",
       "      <td>[&lt;S&gt;, pope, francis, just, called, out, donald...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject               date  \\\n",
       "0    News  December 31, 2017   \n",
       "1    News  December 31, 2017   \n",
       "2    News  December 30, 2017   \n",
       "3    News  December 29, 2017   \n",
       "4    News  December 25, 2017   \n",
       "\n",
       "                                                News  Output  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...       0   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...       0   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...       0   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...       0   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...       0   \n",
       "\n",
       "                                           Tokenized  \n",
       "0  [<S>, donald, trump, sends, out, embarrassing,...  \n",
       "1  [<S>, drunk, bragging, trump, staffer, started...  \n",
       "2  [<S>, sheriff, david, clarke, becomes, an, int...  \n",
       "3  [<S>, trump, is, so, obsessed, he, even, has, ...  \n",
       "4  [<S>, pope, francis, just, called, out, donald...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset['Tokenized'] = tokenize_news(news_dataset['News'])\n",
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Lemmatization\n",
    "The lemmatized dataset will have a specific name ('news_dataset_lemmatized') instead of keeping the same name. This is due to the fact that some methods that we will employ i.e. unigram and bigram method do not benefit from a lemmatized dataset. It is safest to keep the two separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_news(news):  \n",
    "    lemmatized_news = [] \n",
    "\n",
    "    news_to_lemmatize = sp(news.to_string)\n",
    "    lemmatizer = sp.vocab.morphology.lemmatizer\n",
    "    for i in news:\n",
    "        lemmatized_news.append(lemmatizer(i.text, i.pos_))\n",
    "    return lemmas\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'method' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-af13d1f415ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnews_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Lemmatized'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatize_news\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'News'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnews_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-74-cb15274dfed6>\u001b[0m in \u001b[0;36mlemmatize_news\u001b[1;34m(news)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mlemmatized_news\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mnews_to_lemmatize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnews\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;31m#call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         \"\"\"\n\u001b[1;32m--> 426\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m             raise ValueError(\n\u001b[0;32m    428\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'method' has no len()"
     ]
    }
   ],
   "source": [
    "news_dataset['Lemmatized'] = lemmatize_news(news_dataset['News'])\n",
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Splitting into unigrams and bigrams\n",
    "First, we will prepare the token list for each type of ngrams and count the frequency of each structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = list(chain.from_iterable(news_dataset['Tokenized']))\n",
    "bigrams = list(ngrams(chain.from_iterable(news_dataset['Tokenized']), 2))\n",
    "\n",
    "def get_frequencies(ngrams):\n",
    "    ngram_frequencies = {}\n",
    "    for ngram in ngrams:\n",
    "        ngram_frequencies [ngram]=str(ngrams.count(ngram))\n",
    "    return ngram_frequencies\n",
    "\n",
    "unigram_frequencies = get_frequencies(unigrams)\n",
    "bigram_frequencies = get_frequencies(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Splitting into bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
